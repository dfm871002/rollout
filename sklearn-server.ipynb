{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879cda88-6a7b-48e8-b3e8-ae934e319196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "\n",
    "kserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='KServe pipeline',\n",
    "    description='A pipeline for KServe.'\n",
    ")\n",
    "def KServePipeline(\n",
    "    action='apply',\n",
    "    model_name='sklearnserver',\n",
    "    namespace='kubeflow-user-example-com',\n",
    "    custom_model_spec='{\"name\": \"sklearnserver\", \"image\": \"dfm871002/sklearnserver-default:latest\", \"port\": \"5000\"}'\n",
    "):\n",
    "    kserve_op(action=action,\n",
    "              model_name=model_name,\n",
    "              namespace=namespace,\n",
    "              custom_model_spec=custom_model_spec)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(KServePipeline, 'sklsv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c1334d-880e-4d67-b5b9-370fd5349134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to ./...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "489it [00:00, 4879.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiment GOOD total samples 5000\n",
      "negative sentiment  Bad total samples 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 4714.54it/s]\n",
      "5000it [00:01, 4569.41it/s]\n",
      "1it [00:00, 3949.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input sentence is: good ending\n",
      "{\"predictions\": [1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## the following example use python's request to send restapi requests\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import nltk\n",
    "import joblib\n",
    "import sys\n",
    "from random import shuffle\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from nltk import data\n",
    "\n",
    "message = 'good ending'\n",
    "\n",
    "log_folder = './'\n",
    "\n",
    "data.path.append(log_folder)\n",
    "nltk.download('twitter_samples', download_dir = log_folder)\n",
    "nltk.download('stopwords', download_dir = log_folder)\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(f\"positive sentiment GOOD total samples {len(pos_tweets)}\")\n",
    "print(f\"negative sentiment  Bad total samples {len(neg_tweets)}\")\n",
    "\n",
    "class Preprocess():   \n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "        self.stopwords_en = stopwords.words('english') \n",
    "        self.punctuation_en = string.punctuation\n",
    "        self.stemmer = PorterStemmer()        \n",
    "    def __remove_unwanted_characters__(self, tweet):\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        tweet = re.sub('\\S+@\\S+', '', tweet)\n",
    "        tweet = re.sub(r'\\d+', '', tweet)\n",
    "        return tweet    \n",
    "    def __tokenize_tweet__(self, tweet):        \n",
    "        return self.tokenizer.tokenize(tweet)   \n",
    "    def __remove_stopwords__(self, tweet_tokens):\n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in self.stopwords_en and word not in self.punctuation_en):\n",
    "                tweets_clean.append(word)\n",
    "        return tweets_clean   \n",
    "    def __text_stemming__(self,tweet_tokens):\n",
    "        tweets_stem = [] \n",
    "        for word in tweet_tokens:\n",
    "            stem_word = self.stemmer.stem(word)  \n",
    "            tweets_stem.append(stem_word)\n",
    "        return tweets_stem\n",
    "    def preprocess(self, tweets):\n",
    "        tweets_processed = []\n",
    "        for _, tweet in tqdm(enumerate(tweets)):        \n",
    "            tweet = self.__remove_unwanted_characters__(tweet)            \n",
    "            tweet_tokens = self.__tokenize_tweet__(tweet)            \n",
    "            tweet_clean = self.__remove_stopwords__(tweet_tokens)\n",
    "            tweet_stems = self.__text_stemming__(tweet_clean)\n",
    "            tweets_processed.extend([tweet_stems])\n",
    "        return tweets_processed\n",
    "\n",
    "def extract_features(processed_tweet, bow_word_frequency):\n",
    "    features = np.zeros((1,3))\n",
    "    features[0,0] = 1\n",
    "    for word in processed_tweet:\n",
    "        features[0,1] = bow_word_frequency.get((word, 1), 0) + features[0,1]\n",
    "        features[0,2] = bow_word_frequency.get((word, 0), 0) + features[0,2]\n",
    "    return features\n",
    "\n",
    "text_processor = Preprocess()\n",
    "processed_pos_tweets = text_processor.preprocess(pos_tweets)\n",
    "processed_neg_tweets = text_processor.preprocess(neg_tweets)\n",
    "\n",
    "def build_bow_dict(tweets, labels):\n",
    "    freq = {}\n",
    "    for tweet, label in list(zip(tweets, labels)):\n",
    "        for word in tweet:\n",
    "            freq[(word, label)] = freq.get((word, label), 0) + 1    \n",
    "    return freq\n",
    "\n",
    "labels = [1 for i in range(len(processed_pos_tweets))]\n",
    "labels.extend([0 for i in range(len(processed_neg_tweets))])\n",
    "\n",
    "twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n",
    "bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)\n",
    "\n",
    "data = [message]\n",
    "data = text_processor.preprocess(data)\n",
    "            \n",
    "data_o = str(data)\n",
    "data_o = data_o[2:len(data_o)-2]\n",
    "\n",
    "vect = np.zeros((1, 3))\n",
    "for index, tweet in enumerate(data):\n",
    "    vect[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "\n",
    "formData = {\n",
    "    'instances': vect.tolist()\n",
    "}\n",
    "\n",
    "headers = {\"Cookie\": \"authservice_session=MTY0NzM5ODY3NnxOd3dBTkZkR1RVVkpSRFJTU2tsWlZVNVFTRUZWV0RKT1NGUlVSRWt6VDFaQk5VSkZSa0ZJUzB4WVJsZEVUMEZaU2pOUFNsazFTMEU9fCnroQJLCLGBPY61jCnLrylQac2XHgF4OgUOWOC3aCjX\",\n",
    "           \"Host\": \"sklearnserver.kubeflow-user-example-com.example.com\"}\n",
    "\n",
    "print('Your input sentence is: ' + message)\n",
    "res = requests.post('http://istio-ingressgateway.istio-system/v1/models/model:predict', headers=headers, json=formData)\n",
    "print(res.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
